import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions
tf.random.set_seed(123)

from scipy.integrate import solve_ivp
from scipy.optimize import fsolve 
from .model import initiate_particles, norm_rvs, measurements_pred

############################
# Stochastic Particle Flow # 
############################

def Dai22eq28(Lambda_span, beta, args):
    """Compute and return the anti-derivatives of the conditioning number given by negative Hessian, M, over the interval, Lambda_span, using equation (28) in Dai et al. (2022)."""
    mu, HessianLogPrior, HessianLogLikelihood, U = args
    M                = - (HessianLogPrior + beta * HessianLogLikelihood) 
    M_inv            = tf.linalg.inv(M + U)
    db2_dL2          = - mu * ( tf.linalg.trace(HessianLogLikelihood) * tf.linalg.trace(M_inv) + tf.linalg.trace(M) * tf.linalg.trace(M_inv @ HessianLogLikelihood @ M_inv) )
    return db2_dL2

def initial_solve_err(b0, args):
    """Compute the error of the boundary condition beta(1) = 1 at the initial condition beta(0) = 0 for the ODE solver."""
    sol              = solve_ivp(Dai22eq28, t_span=[0,1], y0=b0, args=(args,))
    return tf.constant([sol.y[0,-1] - 1], dtype=tf.float64) 
 
def final_solve(Lambda, mu, HessianLogPrior, HessianLogLikelihood, U): 
    """Solve the ODE system for beta sepcified by Dai22eq28 over the input interval, Lambdas, given the parameters."""
    args            = (mu, HessianLogPrior, HessianLogLikelihood, U)
    root            = fsolve(initial_solve_err, [1e-6], args=(args,))
    sol             = solve_ivp(Dai22eq28, [0,1], y0=root, args=(args,), t_eval=Lambda) 
    return tf.constant(sol.y[0], dtype=tf.float64)
    
def Dai22eq22(beta, mu, HessianLogPrior, HessianLogLikelihood, Q, U):
    """Compute and return the negative Hessian matrix, Jacobian matrix and stiffness, M, F, and kappa, using equation (22) in Dai et al. (2022)."""
    M               = - (HessianLogPrior + beta * HessianLogLikelihood) 
    kappa           = tf.linalg.trace(M) * tf.linalg.trace(tf.linalg.inv(M + U))
    d_beta          = tf.math.sqrt(2 * mu * kappa)
    F               = 1/2 * Q @ (-M) - d_beta/2 * tf.linalg.inv(-M - U) @ HessianLogLikelihood 
    return M, F, kappa

def stiffness_ratio(F):
    """Compute and return the stiffness ratio using the Jacobian matrix F."""
    ei              = tf.constant(tf.math.real(tf.linalg.eigvals(F)), dtype=tf.float64)
    Ratio           = tf.reduce_max(ei) / tf.reduce_min(ei)
    return Ratio

def post_mean(mx, my, Cx, Cy, U):
    Cx_inv = tf.linalg.inv(Cx + U)
    Cy_inv = tf.linalg.inv(Cy + U)
    return tf.linalg.matvec( tf.linalg.inv(Cx_inv + Cy_inv) , tf.linalg.matvec(Cx_inv, mx) + tf.linalg.matvec(Cy_inv, my) )

def post_cov(Cx, Cy, U):
    Cx_inv = tf.linalg.inv(Cx + U)
    Cy_inv = tf.linalg.inv(Cy + U)
    return tf.linalg.inv(Cx_inv + Cy_inv) 

def JacobiLogNormal(x, mu, P, U):
    """Compute and return the Jacobian of the log of a Normal distribution."""
    return tf.linalg.matvec(tf.linalg.inv(P + U), (x - mu))

def HessianLogNormal(P, U):
    """Compute and return the Hessian of the log of a Normal distribution."""
    return - tf.linalg.inv(P + U) 

def Dai22eq11eq12(M, HessianLogLikelihood, Q, U):
    """Compute and return constant terms, K1 and K2, specified by equations (11) and (12) of Dai et al. (2022)."""
    M_inv           = tf.linalg.inv(-M-U)
    K1              = Q/2 + 1/2 * M_inv @ HessianLogLikelihood @ M_inv 
    K2              = - M_inv
    return K1, K2

def drift_f(K1, K2, JacobiLogP, JacobiLogLikelihood):
    """Compute and return the drift function specified by equation (10) of Dai et al. (2022)."""
    f               = tf.linalg.matvec(K1, JacobiLogP) + tf.linalg.matvec(K2, JacobiLogLikelihood)
    return f

def sde_flow_dynamics(N, ndims, K1, K2, JLL,JLP, dL, w0, wI, q):
    """Compute and return the flow dynamics of the particles using the SDE."""
    dx              = tf.Variable(tf.zeros((N,ndims), dtype=tf.float64))    
    f               = drift_f(K1, K2, JLP, JLL)         
    for i in range(N):        
        dw          = norm_rvs(ndims, w0, dL * wI) 
        dx[i,:].assign( f * dL + tf.linalg.matvec(q, dw) )         
    return dx

def SDE(y, model=None, A=None, B=None, V=None, W=None, N=None, Nstep=None, mu0=None, Sigma0=None, muy=None, mc=None, Q=None, linear=True):
    """
    Compute the estimated states using the SDE given the measurements. 

    Keyword args:
    -------------
    y : tf.Variable of float64 with dimension (nTimes,ndims). The measurements generated by SSM. 
    model: string, optional. The name of the measurement model. Defaults to linear Gaussian "LG" if not provided.A : tf.Tensor of float64 with shape (ndims,ndims), optional. The transition matrix. Defaults to diagonal matrix of 0.5 if not provided.
    A : tf.Tensor of float64 with shape (ndims,ndims), optional. The transition matrix. Defaults to diagonal matrix if not provided.
    B : tf.Tensor of float64 with shape (ndims,ndims), optional. The output matrix. Defaults to identity matrix if not provided.
    V : tf.Tensor of float64 with shape (ndims,ndims), optional. The system noise matrix. Defaults to identity matrix if not provided.
    W : tf.Tensor of float64 with shape (ndims,ndims)., optional. The measurement noise matrix. Defaults to identity matrix if not provided.
    N : int32, optional. Number of particles. Defaults to 1000 if not provided.
    Nstep : int32, optional. Number of steps. Defaults to 30 if not provided.
    mu0 : tf.Tensor of float64 with shape (ndims,), optioanl. The prior mean for initial state. Defaults to zeros if not provided.
    Sigma0 : tf.Tensor of float64 with shape (ndims,ndims). The prior covariance for initial state. Defaults to predefined covariances if not provided.
    muy : tf.Tensor of float64 with shape (ndims,), optioanl. The scalar means of the measurements. Defaults to zeros if not provided.
    
    Returns:
    --------
    X_filtered : tf.Variable of float64 with dimension (nTimes,ndims). The filtered states given by the SDE. 
    """
    
    nTimes, ndims   = y.shape     
    
    model           = "LG" if model is None else model
    if model == "SV" and A is None: 
        A           = tf.eye(ndims, dtype=tf.float64) * 0.5  
    elif model == "SV" and A is not None:
        if tf.reduce_max(A) > 1.0 or tf.reduce_min(A) < -1.0:
            raise ValueError("The matrix A out of range [-1,1].")
    if model != "SV" and A is None: 
        A           = tf.eye(ndims, dtype=tf.float64) 
    
    B               = tf.eye(ndims, dtype=tf.float64) if B is None else B
    V               = tf.eye(ndims, dtype=tf.float64) if V is None else V 
    W               = tf.eye(ndims, dtype=tf.float64) if W is None else W
    
    mu0             = tf.zeros((ndims,), dtype=tf.float64)         
    if model == "SV" and Sigma0 is None :
        Sigma0      = V @ tf.linalg.inv(tf.eye(ndims, dtype=tf.float64) - A @ A)  
    elif model != "SV" and Sigma0 is None: 
        Sigma0      = V
        
    muy             = tf.zeros((ndims,), dtype=tf.float64) if muy is None else muy
    mc              = 0.2 if mc is None else mc
    Q               = tf.eye(ndims, dtype=tf.float64) if Q is None else Q
    q               = tf.linalg.cholesky(Q)
    w0              = tf.zeros((ndims,), dtype=tf.float64)
    I               = tf.eye(ndims, dtype=tf.float64)
    
    N               = 1000 if N is None else N
    Np              = N
    Nstep           = 100 if Nstep is None else Nstep
    Rates           = tf.constant(tf.linspace(0.0, 1.0, Nstep + 1).numpy(), dtype=tf.float64)
    u               = tf.eye(ndims, dtype=tf.float64) * 1e-9
    
    CovPost         = post_cov(Sigma0, W, u)
    Hess_LLike      = HessianLogNormal(W, u)
    Hess_Lprior     = HessianLogNormal(Sigma0, u)
    
    if linear:
        betas       = Rates[1:] 
    else:
        betas       = final_solve(Rates[1:], mc, Hess_Lprior, Hess_LLike, u)     

    Ms              = tf.Variable(tf.zeros((Nstep,ndims,ndims), dtype=tf.float64))
    Fs              = tf.Variable(tf.zeros((Nstep,ndims,ndims), dtype=tf.float64))
    cond_num        = tf.Variable(tf.zeros((Nstep,), dtype=tf.float64))
    stiffness       = tf.Variable(tf.zeros((Nstep,), dtype=tf.float64))
    K1s             = tf.Variable(tf.zeros((Nstep,ndims,ndims), dtype=tf.float64))
    K2s             = tf.Variable(tf.zeros((Nstep,ndims,ndims), dtype=tf.float64))
    
    for j in range(Nstep): 
        M, F, k     = Dai22eq22(betas[j], mc, Hess_Lprior, Hess_LLike, Q, u)
        sr          = stiffness_ratio(F)
        K1, K2      = Dai22eq11eq12(M, Hess_LLike, Q, u)
        
        Ms[j,:,:].assign(M)
        Fs[j,:,:].assign(F)
        cond_num[j].assign(k)
        stiffness[j].assign(sr)
        K1s[j,:,:].assign(K1)
        K2s[j,:,:].assign(K2)

    X_filtered      = tf.Variable(tf.zeros((nTimes, ndims), dtype=tf.float64))
    x_filt          = mu0
    
    for i in range(nTimes):
        
        x_prev      = initiate_particles(Np, ndims, x_filt, Sigma0)
        
        gx          = measurements_pred(model, ndims, muy, B, x_filt, W, u)
        mu_p        = post_mean(mu0, gx, Sigma0, W, u)
        JLL         = JacobiLogNormal(y[i,:], gx, W, u)
        JL0         = JacobiLogNormal(x_filt, mu0, Sigma0, u)
        JL1         = JacobiLogNormal(x_filt, mu_p, CovPost, u)
        
        for j in range(Nstep):      
            JLP     = (1.0 - betas[j]) * JL0 + betas[j] * JL1  
            dL      = Rates[j+1] - Rates[j]                   
            dx      = sde_flow_dynamics(N, ndims, K1s[j,:,:], K2s[j,:,:], JLL, JLP, dL, w0, I, q)
            x_prev.assign_add(dx)
            
        x_filt      = tf.reduce_mean(x_prev, axis=0)
        X_filtered[i,:].assign(x_filt)
        
    return X_filtered, cond_num, stiffness, betas


####################################
# Stochastic Particle Flow in LEDH # 
####################################

def LEDH_SDE_Hessians(N, SigmaX, y, ypred, SigmaY, U):
    """Compute and return the Hessians and Jacobian matricies of the log of the Normal distributions for the psuedo particles in LEDH."""
    Jacob_LLike     = tf.TensorArray(tf.float64, size=N, dynamic_size=True, clear_after_read=False)
    Hess_LLike      = tf.TensorArray(tf.float64, size=N, dynamic_size=True, clear_after_read=False)
    Hess_Lprior     = tf.TensorArray(tf.float64, size=N, dynamic_size=True, clear_after_read=False)
    for i in range(N):   
        Hess_LLike  = Hess_LLike.write(i, HessianLogNormal(SigmaY[i,:,:], U))
        Hess_Lprior = Hess_Lprior.write(i, HessianLogNormal(SigmaX[i,:,:], U))
        Jacob_LLike = Jacob_LLike.write(i, JacobiLogNormal(y, ypred[i,:], SigmaY[i,:,:], U))
    return Hess_Lprior.stack(), Hess_LLike.stack(), Jacob_LLike.stack()

def LEDH_SDE_flow_dynamics(N, n, eta0, eta1, SigmaX, ypred, SigmaY, beta, dL, HessPrior, HessLike, JacobLike, mc, w0, wI, Q, q, U):
    """Compute and return the flow dynamics of the pseudo particles for migration in LEDH using the stochastic differential equations."""
    deta            = tf.Variable(tf.zeros((N,n), dtype=tf.float64))     
    prod            = tf.Variable(tf.zeros((N,), dtype=tf.float64))    
    for i in range(N):        
        M, _, _     = Dai22eq22(beta, mc, HessPrior[i,:,:], HessLike[i,:,:], Q, U)
        K1, K2      = Dai22eq11eq12(M, HessLike[i,:,:], Q, U)
        
        JacobPrior  = JacobiLogNormal(eta1[i,:], eta0[i,:], SigmaX[i,:,:], U)
        muPost      = post_mean(eta0[i,:], ypred[i,:], SigmaX[i,:,:], SigmaY[i,:,:], U)
        covPost     = post_cov(SigmaX[i,:,:], SigmaY[i,:,:], U)
        JacobPost   = JacobiLogNormal(eta1[i,:], muPost, covPost, U)
        JLP         = (1.0 - beta) * JacobPrior + beta * JacobPost  
        
        f           = drift_f(K1, K2, JLP, JacobLike[i,:])
        dw          = norm_rvs(n, w0, dL * wI) 
        
        deta[i,:].assign( f * dL + tf.linalg.matvec(q, dw) ) 
        prod[i].assign( tf.math.abs(dL * tf.math.reduce_prod(1 + f)) ) 

    return deta, prod


##########################
# Soft resampling for PF # 
##########################

def LogSumExp(x):
    """Compute and return the log-sum-exponential of the input tensor, x."""
    c               = tf.reduce_max(x) 
    return c + tf.math.log(tf.reduce_sum(tf.math.exp(x - c), axis=0))

def soft_resample(N, x, w):
    """Resample from the set of particles, x, using the weights, w, as multinomial probabilities and return the new set of particles, xbar, and the new weights, wbar.""" 
    Lamb            = tf.cast(tfd.Uniform().sample(), tf.float64)
    what            = Lamb * w + (1-Lamb) / N 
    indices         = tfd.Categorical(probs=what).sample(N)
    xbar            = tf.gather(x, indices)
    wbar            = w / what
    return xbar, wbar 


########################
# OT resampling for PF # 
########################

def cost_matrix(vectors1, vectors2):
    """Compute and return the cost matrix base on euclidean distances for optimal transport."""
    v1              = tf.expand_dims(vectors1, 1)
    v2              = tf.expand_dims(vectors2, 0)
    differences     = v1 - v2
    M               = tf.norm(differences, ord='euclidean', axis=-1)
    return M / tf.reduce_max(M)


def sinkhorn_tf(a, b, C, reg=0.1, num_iter=100):
    """Compute and return the optimal transport matrix."""
    log_K = - C / reg
    
    u = tf.ones_like(a)
    v = tf.ones_like(b)
    log_u = tf.zeros_like(a) 
    log_v = tf.zeros_like(b) 
    
    for i in range(num_iter):
        log_u = tf.math.log(a) - LogSumExp(log_K + log_v[:,tf.newaxis]) 
        log_v = tf.math.log(b) - LogSumExp(log_K + log_u[:, tf.newaxis] + log_v[:, tf.newaxis])
        u = tf.math.exp(log_u)
        v = tf.math.exp(log_v)
        
    P = tf.linalg.diag(u) @ tf.math.exp(log_K) @ tf.linalg.diag(v)
    return P    
    
def ot_resample(N, x, w, C):
    """Resample from the set of particles, x, using the weights, w, as multinomial probabilities and return the new set of particles, xbar, and the new weights, wbar."""
    Lamb            = tf.cast(tfd.Uniform().sample(), tf.float64)
    what            = Lamb * w + (1-Lamb) / N
    POT             = sinkhorn_tf(w, what, C)
    xbar            = POT @ x
    wbar            = w / what
    return xbar, wbar 

# def ot_cv_score(w, x, xpred, SigmaX_inv, SigmaX_det):
#     xdiff           = x - xpred
#     xSum            = - 1/2 * tf.reduce_sum(xdiff @ SigmaX_inv * xdiff, axis=-1)      
#     loglike         = w * ( SigmaX_det + LogSumExp(xSum) ) 
#     return tf.reduce_sum(loglike)

# def ot_grid(num_reg=100):
#     x               = tf.cast(tf.linspace(1e-3, 1, num_reg), tf.float64)
#     return x


